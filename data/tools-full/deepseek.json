{
  "id": "deepseek",
  "displayName": "DeepSeek",
  "auth": {
    "type": "secret_text"
  },
  "actions": {
    "ask_deepseek": {
      "displayName": "Ask Deepseek",
      "description": "Ask Deepseek anything you want!",
      "props": [
        {
          "name": "model",
          "displayName": "Model",
          "type": "DROPDOWN",
          "required": true,
          "description": "The model which will generate the completion."
        },
        {
          "name": "prompt",
          "displayName": "Question",
          "type": "LONG_TEXT",
          "required": true,
          "description": ""
        },
        {
          "name": "frequencyPenalty",
          "displayName": "Frequency penalty",
          "type": "NUMBER",
          "required": false,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
        },
        {
          "name": "maxTokens",
          "displayName": "Maximum Tokens",
          "type": "NUMBER",
          "required": true,
          "description": "The maximum number of tokens to generate. Possible values are between 1 and 8192."
        },
        {
          "name": "presencePenalty",
          "displayName": "Presence penalty",
          "type": "NUMBER",
          "required": false,
          "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the mode's likelihood to talk about new topics."
        },
        {
          "name": "responseFormat",
          "displayName": "Response Format",
          "type": "STATIC_DROPDOWN",
          "required": true,
          "description": "The format of the response. IMPORTANT: When using JSON Output, you must also instruct the model to produce JSON yourself"
        },
        {
          "name": "temperature",
          "displayName": "Temperature",
          "type": "NUMBER",
          "required": false,
          "description": "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Between 0 and 2. We generally recommend altering this or top_p but not both."
        },
        {
          "name": "topP",
          "displayName": "Top P",
          "type": "NUMBER",
          "required": false,
          "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. Values <=1. We generally recommend altering this or temperature but not both."
        },
        {
          "name": "memoryKey",
          "displayName": "Memory Key",
          "type": "SHORT_TEXT",
          "required": false,
          "description": "A memory key that will keep the chat history shared across runs and flows. Keep it empty to leave Deepseek without memory of previous messages."
        },
        {
          "name": "roles",
          "displayName": "Roles",
          "type": "JSON",
          "required": false,
          "description": "Array of roles to specify more accurate response"
        }
      ]
    }
  },
  "triggers": {}
}